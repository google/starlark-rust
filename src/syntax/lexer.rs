// Copyright 2018 The Starlark in Rust Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use std::str::CharIndices;
use std::collections::LinkedList;
use std::collections::linked_list::IntoIter;
use std::iter::Peekable;
use std::mem;
use std::fmt;
use std::char;
use codemap::Span;
use codemap_diagnostic::{Level, SpanLabel, SpanStyle, Diagnostic};
use super::errors::SyntaxError;

// TODO: move that code in some common error code list?
// CL prefix = Critical Lexing
const LEX_ERROR_CODE: &'static str = "CL00";
const INDENT_ERROR_CODE: &'static str = "CL01";
const UNFINISHED_STRING_LITTERAL_CODE: &'static str = "CL02";
const INVALID_ESCAPE_SEQUENCE_CODE: &'static str = "CL03";

/// Errors that can be generated during lexing
#[doc(hidden)]
#[derive(Debug, Clone, PartialEq, Copy)]
pub enum LexerError {
    Indentation(u64, u64),
    InvalidCharacter(u64),
    UnfinishedStringLitteral(u64, u64),
    InvalidEscapeSequence(u64, u64),
    WrappedError {
        span: Span,
        code: &'static str,
        label: &'static str,
    },
}

impl SyntaxError for LexerError {
    /// Convert the error to a codemap diagnostic.
    ///
    /// To build this diagnostic, the method needs the file span corresponding to the parsed
    /// file.
    fn to_diagnostic(self, file_span: Span) -> Diagnostic {
        let sl = SpanLabel {
            span: match self {
                LexerError::Indentation(x, y) |
                LexerError::UnfinishedStringLitteral(x, y) |
                LexerError::InvalidEscapeSequence(x, y) => file_span.subspan(x, y),
                LexerError::InvalidCharacter(x) => file_span.subspan(x, x),
                LexerError::WrappedError { span, .. } => span,
            },
            style: SpanStyle::Primary,
            label: Some(
                match self {
                    LexerError::Indentation(..) => "Incorrect indentation",
                    LexerError::InvalidCharacter(..) => "Character not valid at present location",
                    LexerError::UnfinishedStringLitteral(..) => "Unfinished string litteral",
                    LexerError::InvalidEscapeSequence(..) => "Invalid string escape sequence",
                    LexerError::WrappedError { label, .. } => label,
                }.to_owned(),
            ),
        };
        Diagnostic {
            level: Level::Error,
            message: "Parse error".to_owned(),
            code: Some(
                match self {
                    LexerError::Indentation(..) => INDENT_ERROR_CODE,
                    LexerError::InvalidCharacter(..) => LEX_ERROR_CODE,
                    LexerError::UnfinishedStringLitteral(..) => UNFINISHED_STRING_LITTERAL_CODE,
                    LexerError::InvalidEscapeSequence(..) => INVALID_ESCAPE_SEQUENCE_CODE,
                    LexerError::WrappedError { code, .. } => code,
                }.to_owned(),
            ),
            spans: vec![sl],
        }
    }
}

/// All token that can be generated by the lexer
#[doc(hidden)]
#[derive(Debug, Clone, PartialEq)]
pub enum Token {
    // Indentation block & meaningfull spaces
    Indent, // New indentation block
    Dedent, // Leaving an indentation block
    Newline, // Newline outside a string
    // Keywords
    And, // "and" keyword
    Else, // "else" keyword
    Load, // "load" keyword
    Break, // "break" keyword
    For, // "for" keyword
    Not, // "not" keyword
    NotIn, // "not in" keyword (taken as keyword)
    Continue, // "continue" keyword
    If, // "if" keyword
    Or, // "or" keyword
    Def, // "def" keyword
    In, // "in" keyword
    Pass, // "pass" keyword
    Elif, // "elif" keyword
    Return, // "return" keyword
    // Symbols
    Comma, // ','
    Semicolon, // ';'
    Colon, // ':'
    PlusEqual, // '+='
    MinusEqual, // '-='
    StarEqual, // '*='
    SlashEqual, // '/='
    PercentEqual, // '%='
    DoubleEqual, // '=='
    BangEqual, // '!='
    LowerEqual, // '<='
    GreaterEqual, // '>='
    Doublestar, // '**'
    Equal, // '='
    LowerThan, // '<'
    GreaterThan, // '>'
    Minus, // '-'
    Plus, // '+'
    Star, // '*'
    Percent, // '%'
    Slash, // '/'
    Dot, // '.'
    Pipe, // '|'
    // Brackets
    OpeningBracket, // '['
    OpeningCurlyBracket, // '{'
    OpeningParenthesis, // '('
    ClosingBracket, // ']'
    ClosingCurlyBracket, // '}'
    ClosingParenthesis, // ')'

    Reserved(String), // One of the reserved keywords
    Identifier(String), // An identifier
    IntegerLitteral(i64), // An integer litteral (123, 0x1, 0b1011, 0755, ...)
    StringLitteral(String), // A string litteral
}

impl fmt::Display for Token {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            &Token::Indent => write!(f, "new indentation block"),
            &Token::Dedent => write!(f, "end of indentation block"),
            &Token::Newline => write!(f, "new line"),
            &Token::And => write!(f, "keyword 'and'"),
            &Token::Else => write!(f, "keyword 'else'"),
            &Token::Load => write!(f, "keyword 'load'"),
            &Token::Break => write!(f, "keyword 'break'"),
            &Token::For => write!(f, "keyword 'for'"),
            &Token::Not => write!(f, "keyword 'not'"),
            &Token::NotIn => write!(f, "keyword 'not in'"),
            &Token::Continue => write!(f, "keyword 'continue'"),
            &Token::If => write!(f, "keyword 'if'"),
            &Token::Or => write!(f, "keyword 'or'"),
            &Token::Def => write!(f, "keyword 'def'"),
            &Token::In => write!(f, "keyword 'in'"),
            &Token::Pass => write!(f, "keyword 'pass'"),
            &Token::Elif => write!(f, "keyword 'elif'"),
            &Token::Return => write!(f, "keyword 'return'"),
            &Token::Comma => write!(f, "symbol ','"),
            &Token::Semicolon => write!(f, "symbol ';'"),
            &Token::Colon => write!(f, "symbol ':'"),
            &Token::PlusEqual => write!(f, "symbol '+='"),
            &Token::MinusEqual => write!(f, "symbol '-='"),
            &Token::StarEqual => write!(f, "symbol '*='"),
            &Token::SlashEqual => write!(f, "symbol '/='"),
            &Token::PercentEqual => write!(f, "symbol '%='"),
            &Token::DoubleEqual => write!(f, "symbol '=='"),
            &Token::BangEqual => write!(f, "symbol '!='"),
            &Token::LowerEqual => write!(f, "symbol '<='"),
            &Token::GreaterEqual => write!(f, "symbol '>='"),
            &Token::Doublestar => write!(f, "symbol '**'"),
            &Token::Equal => write!(f, "symbol '='"),
            &Token::LowerThan => write!(f, "symbol '<'"),
            &Token::GreaterThan => write!(f, "symbol '>'"),
            &Token::Minus => write!(f, "symbol '-'"),
            &Token::Plus => write!(f, "symbol '+'"),
            &Token::Star => write!(f, "symbol '*'"),
            &Token::Percent => write!(f, "symbol '%'"),
            &Token::Slash => write!(f, "symbol '/'"),
            &Token::Dot => write!(f, "symbol '.'"),
            &Token::Pipe => write!(f, "symbol '|'"),
            &Token::OpeningBracket => write!(f, "symbol '['"),
            &Token::OpeningCurlyBracket => write!(f, "symbol '{{'"),
            &Token::OpeningParenthesis => write!(f, "symbol '('"),
            &Token::ClosingBracket => write!(f, "symbol ']'"),
            &Token::ClosingCurlyBracket => write!(f, "symbol '}}'"),
            &Token::ClosingParenthesis => write!(f, "symbol ')'"),
            &Token::Reserved(ref s) => write!(f, "reserved keyword '{}'", s),
            &Token::Identifier(ref s) => write!(f, "identifier '{}'", s),
            &Token::IntegerLitteral(ref i) => write!(f, "integer litteral '{}'", i),
            &Token::StringLitteral(ref s) => write!(f, "string litteral '{}'", s),
        }
    }
}

#[doc(hidden)]
pub type LexerItem = Result<(u64, Token, u64), LexerError>;
#[doc(hidden)]
pub trait LexerIntoIter<T: Iterator<Item = LexerItem>>
    : IntoIterator<Item = LexerItem, IntoIter = T> {
}
impl<
    T1: Iterator<Item = LexerItem>,
    T2: IntoIterator<Item = LexerItem, IntoIter = T1>,
> LexerIntoIter<T1> for T2 {
}

/// An iterator over a string slice that convert it to a list of token, i.e. the lexer.
#[derive(Debug)]
#[doc(hidden)]
pub struct Lexer {
    input: String,
    chars: Peekable<CharIndices<'static>>,
    offset: u64,
    process_end_of_file: bool,
    last_new_line: bool,
    last_pos: u64,
    last_next: Option<(u64, char)>,
    indentation_stack: LinkedList<u32>,
    parentheses: i32,
    backlog: LinkedList<LexerItem>,
}

/// An iterator that buffer a Lexer in order to wait for end of block / parentheses.
/// Two consecutive new lines are considered also the end of input to buffer.
#[doc(hidden)]
pub struct BufferedLexer {
    backlog: LinkedList<LexerItem>,
    lexer: Lexer,
    last_colon: bool,
}

impl BufferedLexer {
    pub fn new(input: &str) -> Self {
        let mut r = BufferedLexer {
            backlog: LinkedList::new(),
            lexer: Lexer::new(input),
            last_colon: false,
        };
        r.lexer.process_eof(false);
        r.consume();
        r
    }

    fn consume(&mut self) {
        loop {
            match self.lexer.next() {
                Some(Ok((i, Token::Colon, j))) => {
                    self.last_colon = true;
                    self.backlog.push_back(Ok((i, Token::Colon, j)));
                }
                Some(Ok((i, Token::Newline, j))) => {
                    self.backlog.push_back(Ok((i, Token::Newline, j)));
                }
                Some(x) => {
                    self.last_colon = false;
                    self.backlog.push_back(x);
                }
                None => return,
            }
        }
    }

    pub fn need_more(&self) -> bool {
        self.last_colon || !self.lexer.indentation_stack.is_empty() || self.lexer.parentheses > 0
    }

    pub fn input(&mut self, input: &str) {
        if input.is_empty() || (input.len() == 1 && Lexer::is_nl(input.chars().next().unwrap())) {
            self.lexer.process_eof(true);
        }
        self.lexer.replace_input(input);
        self.consume();
    }
}

impl IntoIterator for BufferedLexer {
    type Item = LexerItem;
    type IntoIter = IntoIter<LexerItem>;

    fn into_iter(self) -> Self::IntoIter {
        self.backlog.into_iter()
    }
}

impl Lexer {
    /// Create a new lexer from a string slice
    pub fn new(input: &str) -> Self {
        let input = input.to_owned();
        let chars = unsafe { mem::transmute(input.char_indices().peekable()) };
        Lexer {
            input,
            chars,
            offset: 0,
            process_end_of_file: true,
            last_new_line: true,
            last_pos: 0,
            last_next: None,
            indentation_stack: LinkedList::new(),
            parentheses: 0,
            backlog: LinkedList::new(),
        }
    }

    /// Mark this Lexer to process or not the end of iterator as end of file
    fn process_eof(&mut self, process: bool) {
        self.process_end_of_file = process
    }

    /// Replace the input by a new one, useful in interactive mode.
    fn replace_input(&mut self, input: &str) {
        self.offset = if let Some((p, _)) = self.peek() {
            p
        } else {
            if let Some((i, c)) = self.last_next {
                i + (c.len_utf8() as u64)
            } else {
                self.last_pos
            }
        };
        assert!(self.offset >= self.last_pos);
        self.input = input.to_owned();
        self.chars = unsafe { mem::transmute(self.input.char_indices().peekable()) };
    }

    /// Enqueue a
    fn is_nl(c: char) -> bool {
        match c {
            '\n' | '\r' | '\u{2028}' | '\u{2029}' => true,
            _ => false,
        }
    }

    fn peek(&mut self) -> Option<(u64, char)> {
        match self.chars.peek() {
            Some(&(x, c)) => Some((x as u64 + self.offset, c)),
            None => None,
        }
    }

    fn pop(&mut self) -> Option<(u64, char)> {
        let r = self.chars.next();
        self.last_next = match r {
            Some((x, c)) => {
                self.last_new_line = Lexer::is_nl(c);
                Some((x as u64 + self.offset, c))
            }
            None => {
                self.last_new_line = false;
                None
            }
        };
        self.last_next
    }

    fn terminate(&mut self) {
        (&mut self.chars).last();
        self.indentation_stack.clear();
        self.parentheses = 0;
    }

    fn next_char(&mut self) -> char {
        self.pop().unwrap_or((0, '\0')).1
    }

    fn peek_char(&mut self) -> char {
        self.peek().unwrap_or((0, '\0')).1
    }

    fn return_none(&mut self) -> Option<<Self as Iterator>::Item> {
        // Emit a newline and N DEDENT at EOF
        let p = self.end_pos();
        if !self.last_new_line {
            self.last_new_line = true;
            Some(Ok((p.1, Token::Newline, p.1)))
        } else if self.ihead() > 0 && self.process_end_of_file {
            self.indentation_stack.pop_front();
            Some(Ok((p.1, Token::Dedent, p.1)))
        } else {
            None
        }
    }

    fn ihead(&self) -> u32 {
        if self.indentation_stack.is_empty() {
            0
        } else {
            *self.indentation_stack.front().unwrap()
        }
    }

    fn begin(&mut self) {
        if let Some((i, ..)) = self.peek() {
            self.last_pos = i;
        }
    }

    fn end_pos(&mut self) -> (u64, u64) {
        if let Some((end, ..)) = self.peek() {
            (self.last_pos, end)
        } else {
            if let Some((i, c)) = self.last_next {
                (self.last_pos, i + (c.len_utf8() as u64))
            } else {
                (self.last_pos, self.last_pos)
            }
        }
    }

    fn end(&mut self, res: Token) -> Option<<Self as Iterator>::Item> {
        let p = self.end_pos();
        assert!(p.0 <= p.1, "{} > {}", p.0, p.1);
        Some(Ok((p.0, res, p.1)))
    }

    fn consume(&mut self, res: Token) -> Option<<Self as Iterator>::Item> {
        self.pop();
        self.end(res)
    }

    fn invalid(&mut self) -> Option<<Self as Iterator>::Item> {
        let p = self.end_pos();
        Some(Err(LexerError::InvalidCharacter(p.1)))
    }

    fn internal_next(&mut self) -> Option<<Self as Iterator>::Item> {
        if !self.backlog.is_empty() {
            return self.backlog.pop_front();
        }
        if self.peek().is_none() {
            return self.return_none();
        }
        let r = self.consume_token();
        if let Some(Err(_)) = r {
            // In case of errors, consume the whole input so we stop on next call
            self.terminate();
        } else if r.is_none() {
            return self.return_none();
        }
        r
    }
}

impl Iterator for Lexer {
    type Item = LexerItem;

    #[cfg(feature = "trace")]
    fn next(&mut self) -> Option<Self::Item> {
        let r = self.internal_next();
        println!("[TOKEN] {:?}", r);
        r
    }

    #[cfg(not(feature = "trace"))]
    fn next(&mut self) -> Option<Self::Item> {
        self.internal_next()
    }
}

// Consumers to actually consume token
impl Lexer {
    fn token_from_identifier(identifier: &str) -> Token {
        match identifier {
            "and" => Token::And,
            "else" => Token::Else,
            "load" => Token::Load,
            "break" => Token::Break,
            "for" => Token::For,
            "not" => Token::Not,
            "continue" => Token::Continue,
            "if" => Token::If,
            "or" => Token::Or,
            "def" => Token::Def,
            "in" => Token::In,
            "pass" => Token::Pass,
            "elif" => Token::Elif,
            "return" => Token::Return,
            "as" | "import" | "assert" | "is" | "class" | "nonlocal" | "del" | "raise" |
            "except" | "try" | "finally" | "while" | "from" | "with" | "global" | "yield" => {
                Token::Reserved(identifier.to_owned())
            }
            _ => Token::Identifier(identifier.to_owned()),
        }
    }

    fn skip_comment(&mut self) {
        assert_eq!(self.next_char(), '#');
        loop {
            match self.peek_char() {
                '\n' | '\r' | '\u{2028}' | '\u{2029}' | '\0' => return,
                _ => {
                    self.pop();
                }
            }
        }
    }

    fn skip_spaces(&mut self, newline: bool) {
        loop {
            match self.peek_char() {
                '\n' | '\r' | '\u{2028}' | '\u{2029}' => {
                    if newline {
                        self.pop();
                    } else {
                        return;
                    }
                }
                '\t' | ' ' => {
                    self.pop();
                }
                '#' => self.skip_comment(),
                _ => return,
            };
        }
    }

    fn consume_spaces(&mut self) -> u32 {
        let mut result = 0;
        loop {
            match self.peek_char() {
                '\t' => result += 8 - (result % 8),
                ' ' => result += 1,
                _ => return result,
            };
            self.pop();
        }
    }

    fn consume_indentation(&mut self) -> Option<<Self as Iterator>::Item> {
        loop {
            self.begin();
            let spaces = self.consume_spaces();
            let p = self.peek_char();
            if Lexer::is_nl(p) {
                // ignore because it is an empty line, but still return new line
                return None;
            } else if p == '#' {
                // Ignore the comment and start again
                self.skip_comment();
                self.consume_nl();
                continue;
            } else if spaces > self.ihead() {
                self.indentation_stack.push_front(spaces);
                return self.end(Token::Indent);
            } else if spaces == self.ihead() {
                return None;
            } else {
                let mut step = 0;
                while spaces < self.ihead() {
                    self.indentation_stack.pop_front();
                    step += 1;
                }
                if spaces == self.ihead() {
                    let r = self.end(Token::Dedent);
                    while step > 1 {
                        self.backlog.push_front(r.clone().unwrap());
                        step -= 1;
                    }
                    return r;
                } else {
                    let p = self.end_pos();
                    return Some(Err(LexerError::Indentation(p.0, p.1)));
                }
            }
        }
    }

    fn consume_nl(&mut self) -> Option<<Self as Iterator>::Item> {
        self.begin();
        match (self.next_char(), self.peek_char()) {
            ('\n', '\r') | ('\r', '\n') => self.consume(Token::Newline),
            _ => self.end(Token::Newline),
        }
    }

    fn consume_identifier_queue(&mut self, head: &str) -> Option<<Self as Iterator>::Item> {
        let mut result = head.to_owned();
        while self.peek_char().is_alphabetic() || self.peek_char().is_digit(10) ||
            self.peek_char() == '_'
        {
            result.push(self.next_char());
        }
        assert!(!result.is_empty());
        let r = self.end(Self::token_from_identifier(&result));
        match r {
            Some(Ok((b, Token::Not, ..))) => {
                // Special handling of "not in"
                self.consume_spaces();
                if self.peek_char() == 'i' {
                    match self.consume_identifier() {
                        Some(Ok((.., Token::In, e))) => Some(Ok((b, Token::NotIn, e))),
                        Some(next_id) => {
                            self.backlog.push_front(next_id);
                            r
                        }
                        None => r, // This should never happen but it is safe to just return r.
                    }
                } else {
                    r
                }
            }
            _ => r,
        }
    }

    fn consume_identifier(&mut self) -> Option<<Self as Iterator>::Item> {
        self.begin();
        assert!(!self.peek_char().is_digit(10));
        self.consume_identifier_queue("")
    }

    fn consume_int_r(&mut self, radix: u32) -> Result<i64, ()> {
        let mut number = String::new();
        while self.peek_char().is_digit(radix) {
            number.push(self.next_char());
        }
        let val = i64::from_str_radix(&number, radix);
        if val.is_err() {
            Err(())
        } else {
            Ok(val.unwrap())
        }
    }

    fn consume_int_radix(&mut self, radix: u32) -> Option<<Self as Iterator>::Item> {
        let val = self.consume_int_r(radix);
        if self.peek_char().is_alphanumeric() {
            self.invalid()
        } else {
            if val.is_err() {
                self.invalid()
            } else {
                self.end(Token::IntegerLitteral(val.unwrap()))
            }
        }
    }

    fn consume_int(&mut self) -> Option<<Self as Iterator>::Item> {
        self.begin();
        let cur = self.peek_char();
        if cur == '0' {
            self.pop();
            let cur = self.peek_char();
            match cur {
                'o' | 'O' => {
                    self.pop();
                    self.consume_int_radix(8)
                }
                '0'...'7' => self.consume_int_radix(8),
                'x' | 'X' => {
                    self.pop();
                    self.consume_int_radix(16)
                }
                'b' | 'B' => {
                    self.pop();
                    self.consume_int_radix(2)
                }
                c if !c.is_alphanumeric() => self.end(Token::IntegerLitteral(0)),
                _ => self.invalid(),
            }
        } else {
            self.consume_int_radix(10)
        }
    }

    fn consume_escape_sequence(&mut self, triple: bool) -> Result<Option<char>, LexerError> {
        if let Some((pos, c)) = self.pop() {
            assert_eq!(c, '\\');
            if let Some((pos2, c2)) = self.pop() {
                match c2 {
                    'n' => Ok(Some('\n')),
                    'r' => Ok(Some('\r')),
                    't' => Ok(Some('\t')),
                    '0' => {
                        if self.peek_char().is_digit(8) {
                            if let Ok(r) = self.consume_int_r(8) {
                                Ok(Some(char::from_u32(r as u32).unwrap()))
                            } else {
                                let p = self.end_pos();
                                Err(LexerError::InvalidEscapeSequence(pos, p.1))
                            }
                        } else {
                            Ok(Some('\0'))
                        }
                    }
                    'x' => {
                        if let Ok(r) = self.consume_int_r(16) {
                            Ok(Some(char::from_u32(r as u32).unwrap()))
                        } else {
                            let p = self.end_pos();
                            Err(LexerError::InvalidEscapeSequence(pos, p.1))
                        }
                    }
                    '1'...'9' => Err(LexerError::InvalidEscapeSequence(pos, pos2 + 1)),
                    '\n' => {
                        if triple {
                            Ok(None)
                        } else {
                            Err(LexerError::InvalidEscapeSequence(pos, pos2 + 1))
                        }
                    }
                    'u' => {
                        let c = self.next_char();
                        if c != '{' {
                            let p = self.end_pos();
                            Err(LexerError::InvalidEscapeSequence(pos, p.1))
                        } else if let Ok(r) = self.consume_int_r(16) {
                            let c = self.next_char();
                            if c != '}' {
                                let p = self.end_pos();
                                Err(LexerError::InvalidEscapeSequence(pos, p.1))
                            } else {
                                Ok(Some(char::from_u32(r as u32).unwrap()))
                            }
                        } else {
                            let p = self.end_pos();
                            Err(LexerError::InvalidEscapeSequence(pos, p.1))
                        }
                    }
                    x => Ok(Some(x)),
                }
            } else {
                Err(LexerError::InvalidEscapeSequence(pos, pos + 1))
            }
        } else {
            panic!("This is a bug");
        }
    }

    fn consume_string(&mut self, raw: bool) -> Option<<Self as Iterator>::Item> {
        self.begin();
        let mut res = String::new();
        let quote = self.next_char();
        let mut triple = false;
        if self.peek_char() == quote {
            self.next_char();
            if self.peek_char() == quote {
                self.next_char();
                triple = true;
            } else {
                return self.end(Token::StringLitteral(res));
            }
        }
        loop {
            match self.peek_char() {
                '\\' => {
                    if raw {
                        self.pop();
                        if self.peek_char() == quote {
                            self.pop();
                            res.push(quote);
                        } else {
                            res.push('\\');
                        }
                    } else {
                        match self.consume_escape_sequence(triple) {
                            Ok(Some(x)) => res.push(x),
                            Ok(None) => {}
                            Err(c) => return Some(Result::Err(c)),
                        }
                    }
                }
                '\n' | '\r' | '\u{2028}' | '\u{2029}' => {
                    if triple {
                        res.push(self.next_char());
                    } else {
                        let p = self.end_pos();
                        return Some(Err(LexerError::UnfinishedStringLitteral(p.0, p.1)));
                    }
                }
                '\0' => {
                    let p = self.end_pos();
                    return Some(Err(LexerError::UnfinishedStringLitteral(p.0, p.1)));
                }
                x if x == quote => {
                    self.pop();
                    if triple {
                        if self.next_char() == quote {
                            if self.next_char() == quote {
                                break;
                            } else {
                                res.push(quote);
                                res.push(quote);
                            }
                        } else {
                            res.push(quote);
                        }
                    } else {
                        break;
                    }
                }
                x => {
                    self.pop();
                    res.push(x);
                }
            }
        }
        self.end(Token::StringLitteral(res))
    }

    fn consume_token(&mut self) -> Option<<Self as Iterator>::Item> {
        if self.last_new_line && self.parentheses == 0 {
            let r = self.consume_indentation();
            if r.is_some() {
                return r;
            }
        } else {
            let skip_newline = self.parentheses > 0;
            self.skip_spaces(skip_newline);
        }
        self.begin();
        match self.peek_char() {
            '\0' => None,
            '\n' | '\r' | '\u{2028}' | '\u{2029}' => return self.consume_nl(),
            '\'' | '"' => return self.consume_string(false),
            'r' => {
                self.pop();
                let p = self.peek_char();
                if p == '\'' || p == '"' {
                    return self.consume_string(true);
                } else {
                    return self.consume_identifier_queue("r");
                }
            }
            '0'...'9' => return self.consume_int(),
            '_' => return self.consume_identifier(),
            c if c.is_alphabetic() => return self.consume_identifier(),
            ',' => return self.consume(Token::Comma),
            ';' => return self.consume(Token::Semicolon),
            ':' => return self.consume(Token::Colon),
            '+' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::PlusEqual)
                } else {
                    self.end(Token::Plus)
                };
            }
            '-' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::MinusEqual)
                } else {
                    self.end(Token::Minus)
                };
            }
            '*' => {
                self.pop();
                match self.peek_char() {
                    '=' => return self.consume(Token::StarEqual),
                    '*' => return self.consume(Token::Doublestar),
                    _ => return self.end(Token::Star),
                }
            }
            '/' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::SlashEqual)
                } else {
                    self.end(Token::Slash)
                };
            }
            '%' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::PercentEqual)
                } else {
                    self.end(Token::Percent)
                };
            }
            '=' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::DoubleEqual)
                } else {
                    self.end(Token::Equal)
                };
            }
            '!' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::BangEqual)
                } else {
                    self.invalid()
                };
            }
            '<' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::LowerEqual)
                } else {
                    self.end(Token::LowerThan)
                };
            }
            '>' => {
                self.pop();
                return if self.peek_char() == '=' {
                    self.consume(Token::GreaterEqual)
                } else {
                    self.end(Token::GreaterThan)
                };
            }
            '|' => return self.consume(Token::Pipe),
            '.' => return self.consume(Token::Dot),
            '[' => {
                self.parentheses += 1;
                return self.consume(Token::OpeningBracket);
            }
            ']' => {
                self.parentheses -= 1;
                return self.consume(Token::ClosingBracket);
            }
            '(' => {
                self.parentheses += 1;
                return self.consume(Token::OpeningParenthesis);
            }
            ')' => {
                self.parentheses -= 1;
                return self.consume(Token::ClosingParenthesis);
            }
            '{' => {
                self.parentheses += 1;
                return self.consume(Token::OpeningCurlyBracket);
            }
            '}' => {
                self.parentheses -= 1;
                return self.consume(Token::ClosingCurlyBracket);
            }
            _ => return self.invalid(),
        }
    }
}


#[cfg(test)]
mod tests {
    use super::Token;
    use syntax::errors::SyntaxError;
    use std::fs;
    use std::path::PathBuf;
    use std::fs::File;
    use std::io::Read;
    use std::sync::{Arc, Mutex};
    use codemap;
    use codemap_diagnostic;

    fn collect_result_buffered(s: Vec<&'static str>) -> Vec<Token> {
        let codemap = Arc::new(Mutex::new(codemap::CodeMap::new()));
        let mut diagnostics = Vec::new();
        let mut result = Vec::new();
        let content = s.iter().fold("".to_string(), |a, it| a + it);
        let file_span = {
            codemap
                .lock()
                .unwrap()
                .add_file("<test>".to_owned(), content)
                .span
        };
        let mut lexer = super::BufferedLexer::new(s[0]);
        for v in s.iter().skip(1) {
            assert!(
                lexer.need_more(),
                "Should need more before '{}'",
                v,
            );

            lexer.input(&v)
        }
        assert!(!lexer.need_more());
        let mut pos = 0;
        for x in lexer.into_iter() {
            if x.is_err() {
                diagnostics.push(x.err().unwrap().to_diagnostic(file_span));
            } else {
                let (i, t, j) = x.unwrap();
                let span_incorrect = format!("Span of {:?} incorrect", t);
                assert!(pos <= i, "{}: {} > {}", span_incorrect, pos, i);
                result.push(t);
                assert!(i <= j, "{}: {} > {}", span_incorrect, i, j);
                pos = j;
            }
        }
        assert_diagnostics!(diagnostics, codemap);
        result
    }

    fn collect_result(s: &'static str) -> Vec<Token> {
        let codemap = Arc::new(Mutex::new(codemap::CodeMap::new()));
        let mut diagnostics = Vec::new();
        let mut result = Vec::new();
        let file_span = {
            codemap
                .lock()
                .unwrap()
                .add_file("<test>".to_owned(), s.to_owned())
                .span
        };
        let mut pos = 0;
        super::Lexer::new(s).for_each(|x| if x.is_err() {
            diagnostics.push(x.err().unwrap().to_diagnostic(file_span));
        } else {
            let (i, t, j) = x.unwrap();
            let span_incorrect = format!("Span of {:?} incorrect", t);
            assert!(pos <= i, "{}: {} > {}", span_incorrect, pos, i);
            result.push(t);
            assert!(i <= j, "{}: {} > {}", span_incorrect, i, j);
            pos = j;
        });
        assert_diagnostics!(diagnostics, codemap);
        result
    }

    #[test]
    fn test_int_lit() {
        let get_result = |s: &'static str| -> Vec<i64> {
            collect_result(s)
                .iter()
                .filter_map(|v| match v {
                    &Token::IntegerLitteral(r) => Some(r),
                    &Token::Newline => None,
                    _ => panic!("{:?} is not a integer litteral", v),
                })
                .collect()
        };
        assert_eq!(vec![0, 123], get_result("0 123"));
        assert_eq!(vec![0x7f, 0x7f], get_result("0x7F 0x7f"));
        assert_eq!(vec![0b1011, 0b1011], get_result("0B1011 0b1011"));
        assert_eq!(vec![0o755, 0o755, 0o755], get_result("0o755 0O755 0755"));
    }

    #[test]
    fn test_indentation() {
        let r = collect_result(
            "
+
  -
      /
      *
  =
    %
      .
+=
",
        );
        assert_eq!(
            &[
                Token::Newline,
                Token::Plus,
                Token::Newline,
                Token::Indent,
                Token::Minus,
                Token::Newline,
                Token::Indent,
                Token::Slash,
                Token::Newline,
                Token::Star,
                Token::Newline,
                Token::Dedent,
                Token::Equal,
                Token::Newline,
                Token::Indent,
                Token::Percent,
                Token::Newline,
                Token::Indent,
                Token::Dot,
                Token::Newline,
                Token::Dedent,
                Token::Dedent,
                Token::Dedent,
                Token::PlusEqual,
                Token::Newline,
            ],
            &r[..]
        );
    }

    #[test]
    fn test_symbols() {
        let r = collect_result(
            ", ; : += -= *= /= %= == != <= >= ** = < > - + * % / . { } [ ] ( ) |",
        );
        assert_eq!(
            &[
                Token::Comma,
                Token::Semicolon,
                Token::Colon,
                Token::PlusEqual,
                Token::MinusEqual,
                Token::StarEqual,
                Token::SlashEqual,
                Token::PercentEqual,
                Token::DoubleEqual,
                Token::BangEqual,
                Token::LowerEqual,
                Token::GreaterEqual,
                Token::Doublestar,
                Token::Equal,
                Token::LowerThan,
                Token::GreaterThan,
                Token::Minus,
                Token::Plus,
                Token::Star,
                Token::Percent,
                Token::Slash,
                Token::Dot,
                Token::OpeningCurlyBracket,
                Token::ClosingCurlyBracket,
                Token::OpeningBracket,
                Token::ClosingBracket,
                Token::OpeningParenthesis,
                Token::ClosingParenthesis,
                Token::Pipe,
                Token::Newline,
            ],
            &r[..]
        );
    }

    #[test]
    fn test_keywords() {
        let r = collect_result(
            "and else load break for not not  in continue if or def in pass elif return",
        );
        assert_eq!(
            &[
                Token::And,
                Token::Else,
                Token::Load,
                Token::Break,
                Token::For,
                Token::Not,
                Token::NotIn,
                Token::Continue,
                Token::If,
                Token::Or,
                Token::Def,
                Token::In,
                Token::Pass,
                Token::Elif,
                Token::Return,
                Token::Newline,
            ],
            &r[..]
        );
    }

    #[test]
    fn test_reserved() {
        let r = collect_result(
            "as import assert is class nonlocal del raise except try finally \
            while from with global yield",
        );
        assert_eq!(
            &[
                Token::Reserved("as".to_owned()),
                Token::Reserved("import".to_owned()),
                Token::Reserved("assert".to_owned()),
                Token::Reserved("is".to_owned()),
                Token::Reserved("class".to_owned()),
                Token::Reserved("nonlocal".to_owned()),
                Token::Reserved("del".to_owned()),
                Token::Reserved("raise".to_owned()),
                Token::Reserved("except".to_owned()),
                Token::Reserved("try".to_owned()),
                Token::Reserved("finally".to_owned()),
                Token::Reserved("while".to_owned()),
                Token::Reserved("from".to_owned()),
                Token::Reserved("with".to_owned()),
                Token::Reserved("global".to_owned()),
                Token::Reserved("yield".to_owned()),
                Token::Newline,
            ],
            &r[..]
        );
    }

    #[test]
    fn test_comment() {
        // Comment should be ignored
        assert!(collect_result("# a comment\n").is_empty());
        assert!(collect_result(" # a comment\n").is_empty());
        let r = collect_result("a # a comment\n");
        assert_eq!(&[Token::Identifier("a".to_owned()), Token::Newline], &r[..]);
        // But it should not eat everything
        let r = collect_result("[\n# a comment\n]");
        assert_eq!(
            &[Token::OpeningBracket, Token::ClosingBracket, Token::Newline],
            &r[..]
        );
    }

    #[test]
    fn test_identifier() {
        let r = collect_result("a identifier CAPS _CAPS _0123");
        assert_eq!(
            &[
                Token::Identifier("a".to_owned()),
                Token::Identifier("identifier".to_owned()),
                Token::Identifier("CAPS".to_owned()),
                Token::Identifier("_CAPS".to_owned()),
                Token::Identifier("_0123".to_owned()),
                Token::Newline,
            ],
            &r[..]
        );

        assert!(super::Lexer::new("12a").next().unwrap().is_err());
    }

    #[test]
    fn test_string_lit() {
        let r = collect_result("'123' \"123\" '' \"\" '\\'' \"\\\"\" '\"' \"'\" '\\n'");
        assert_eq!(
            &[
                Token::StringLitteral("123".to_owned()),
                Token::StringLitteral("123".to_owned()),
                Token::StringLitteral("".to_owned()),
                Token::StringLitteral("".to_owned()),
                Token::StringLitteral("'".to_owned()),
                Token::StringLitteral("\"".to_owned()),
                Token::StringLitteral("\"".to_owned()),
                Token::StringLitteral("'".to_owned()),
                Token::StringLitteral("\n".to_owned()),
                Token::Newline,
            ],
            &r[..]
        );

        // unfinished string litteral
        assert_eq!(
            super::Lexer::new("'\n'").next().unwrap(),
            Err(super::LexerError::UnfinishedStringLitteral(0, 1))
        );
        assert_eq!(
            super::Lexer::new("\"\n\"").next().unwrap(),
            Err(super::LexerError::UnfinishedStringLitteral(0, 1))
        );
        // Multiline string
        let r = collect_result(
            "'''''' '''\\n''' '''\n''' \"\"\"\"\"\" \"\"\"\\n\"\"\" \"\"\"\n\"\"\"",
        );
        assert_eq!(
            &[
                Token::StringLitteral("".to_owned()),
                Token::StringLitteral("\n".to_owned()),
                Token::StringLitteral("\n".to_owned()),
                Token::StringLitteral("".to_owned()),
                Token::StringLitteral("\n".to_owned()),
                Token::StringLitteral("\n".to_owned()),
                Token::Newline,
            ],
            &r[..]
        );
        // Raw string
        let r = collect_result("r'' r\"\" r'\\'' r\"\\\"\" r'\"' r\"'\" r'\\n'");
        assert_eq!(
            &[
                Token::StringLitteral("".to_owned()),
                Token::StringLitteral("".to_owned()),
                Token::StringLitteral("'".to_owned()),
                Token::StringLitteral("\"".to_owned()),
                Token::StringLitteral("\"".to_owned()),
                Token::StringLitteral("'".to_owned()),
                Token::StringLitteral("\\n".to_owned()),
                Token::Newline,
            ],
            &r[..]
        );

    }

    #[test]
    fn test_simple_example() {
        let r = collect_result(
            "\"\"\"A docstring.\"\"\"

def _impl(ctx):
  # Print Hello, World!
  print('Hello, World!')
",
        );
        assert_eq!(
            &[
                Token::StringLitteral("A docstring.".to_owned()),
                Token::Newline,
                Token::Newline,
                Token::Def,
                Token::Identifier("_impl".to_owned()),
                Token::OpeningParenthesis,
                Token::Identifier("ctx".to_owned()),
                Token::ClosingParenthesis,
                Token::Colon,
                Token::Newline,
                Token::Indent,
                Token::Identifier("print".to_owned()),
                Token::OpeningParenthesis,
                Token::StringLitteral("Hello, World!".to_owned()),
                Token::ClosingParenthesis,
                Token::Newline,
                Token::Dedent,
            ],
            &r[..]
        );
    }

    #[test]
    fn test_buffered() {
        let r = collect_result_buffered(vec!["\"\"\"A docstring.\"\"\"\n"]);
        assert_eq!(
            &[
                Token::StringLitteral("A docstring.".to_owned()),
                Token::Newline,
            ],
            &r[..]
        );
        let r = collect_result_buffered(vec!["\n"]);
        assert_eq!(&[Token::Newline], &r[..]);
        let r = collect_result_buffered(vec![
            "def _impl(ctx):\n",
            "  # Print Hello, World!\n",
            "  print('Hello, World!')\n",
            "\n",
        ]);
        assert_eq!(
            &[
                Token::Def,
                Token::Identifier("_impl".to_owned()),
                Token::OpeningParenthesis,
                Token::Identifier("ctx".to_owned()),
                Token::ClosingParenthesis,
                Token::Colon,
                Token::Newline,
                Token::Indent,
                Token::Identifier("print".to_owned()),
                Token::OpeningParenthesis,
                Token::StringLitteral("Hello, World!".to_owned()),
                Token::ClosingParenthesis,
                Token::Newline,
                Token::Newline,
                Token::Dedent,
            ],
            &r[..]
        );
    }

    #[test]
    fn smoke_test() {
        let mut d = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        let codemap = Arc::new(Mutex::new(codemap::CodeMap::new()));
        let mut diagnostics = Vec::new();

        d.push("src/syntax/testcases");
        let paths = fs::read_dir(d.as_path()).unwrap();
        for p in paths {
            let entry = p.unwrap();
            let filename = entry.file_name().into_string().unwrap();
            if filename.ends_with(".bzl") {
                let mut content = String::new();
                let mut file = File::open(entry.path()).unwrap();
                file.read_to_string(&mut content).unwrap();
                let file_span = {
                    codemap
                        .lock()
                        .unwrap()
                        .add_file(filename, content.clone())
                        .span
                };
                super::Lexer::new(&content).for_each(|x| if x.is_err() {
                    diagnostics.push(x.err().unwrap().to_diagnostic(file_span));
                });
            }
        }
        assert_diagnostics!(diagnostics, codemap);
    }
}
